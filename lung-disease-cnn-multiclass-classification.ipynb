{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7800340,"sourceType":"datasetVersion","datasetId":4567313}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Import and Setup**","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport random\nrandom.seed(42)\n\nimport torch\ntorch.manual_seed(42)\n\nimport numpy as np\nnp.random.seed(42)\n\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-09-26T20:41:22.989955Z","iopub.execute_input":"2024-09-26T20:41:22.990428Z","iopub.status.idle":"2024-09-26T20:41:29.228161Z","shell.execute_reply.started":"2024-09-26T20:41:22.990362Z","shell.execute_reply":"2024-09-26T20:41:29.227066Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset path\ndata_dir = '/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image'\nprint(os.listdir(data_dir))  \n\n# Class labels\nlabel_dict = {'Normal': 0, 'Lung_Opacity': 1, 'Viral Pneumonia': 2} \n\n# Store image paths and their corresponding labels\nimage_paths = []\nlabels = []\n\n# Loop through each class to collect images and labels\nfor class_name in label_dict.keys():\n    class_folder = os.path.join(data_dir, class_name)\n\n    # Check if the folder exists and list files\n    if os.path.exists(class_folder):\n        for filename in os.listdir(class_folder):\n            if filename.endswith('.jpg'):\n                image_paths.append(os.path.join(class_folder, filename))\n                labels.append(label_dict[class_name])\n    else:\n        print(f\"Folder {class_folder} does not exist.\")\n\n# Print a few image paths and labels to verify\nprint(image_paths[:5])  \nprint(labels[:5])       \n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T20:41:29.230239Z","iopub.execute_input":"2024-09-26T20:41:29.230762Z","iopub.status.idle":"2024-09-26T20:41:30.781809Z","shell.execute_reply.started":"2024-09-26T20:41:29.230720Z","shell.execute_reply":"2024-09-26T20:41:30.780285Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['Normal', 'Lung_Opacity', 'Viral Pneumonia']\n['/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image/Normal/623.jpg', '/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image/Normal/764.jpg', '/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image/Normal/1075.jpg', '/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image/Normal/771.jpg', '/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image/Normal/208.jpg']\n[0, 0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Preprocessing**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Dataset splitting into 70% train, 15% validation, and 15% test\n# First, split into 70% train and 30% (validation + test)\ntrain_paths, val_test_paths, train_labels, val_test_labels = train_test_split(\n    image_paths, labels, test_size=0.30, random_state=42, stratify=labels\n)\n\n# Then, split the remaining 30% into 15% validation and 15% test\nval_paths, test_paths, val_labels, test_labels = train_test_split(\n    val_test_paths, val_test_labels, test_size=0.5, random_state=42, stratify=val_test_labels\n)\n\n# Verifying the splits\nprint(f\"Training samples: {len(train_paths)}\")\nprint(f\"Validation samples: {len(val_paths)}\")\nprint(f\"Testing samples: {len(test_paths)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T20:41:30.783204Z","iopub.execute_input":"2024-09-26T20:41:30.783584Z","iopub.status.idle":"2024-09-26T20:41:30.992028Z","shell.execute_reply.started":"2024-09-26T20:41:30.783544Z","shell.execute_reply":"2024-09-26T20:41:30.990892Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Training samples: 2432\nValidation samples: 521\nTesting samples: 522\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18, ResNet18_Weights\n\n# Set device \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Load pre-trained ResNet-18 model\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Freeze the earlier layers of the ResNet model selectively (freeze first few layers)\n# Keep layers from 'layer4' (deeper layers) trainable for fine-tuning\nfor name, param in model.named_parameters():\n    if name.startswith(\"layer4\") or name.startswith(\"fc\"):\n        param.requires_grad = True  # Fine-tune the last layer and deeper blocks\n    else:\n        param.requires_grad = False  # Freeze all earlier layers\n\n# Modify the final FC layer for the 3 classes \nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 3) \n\n# Move model to GPU\nmodel = model.to(device)\n\n# Print model to confirm layer freezing\nprint(\"Layer Freezing Status:\")\nfor name, param in model.named_parameters():\n    print(f'{name}: {\"Trainable\" if param.requires_grad else \"Frozen\"}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T20:41:30.995117Z","iopub.execute_input":"2024-09-26T20:41:30.995569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Augmentation**","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\n# Transforms for data augmentation and normalization\ntrain_transforms = transforms.Compose([\n    transforms.RandomRotation(20),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Standard normalization for pre-trained ResNet\n])\n\nval_test_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Custom dataset class to load images and apply transformations\nclass LungDiseaseDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert('RGB')  # Convert to RGB as medical images are 3-channel\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Create datasets for training, validation, and testing\ntrain_dataset = LungDiseaseDataset(train_paths, train_labels, transform=train_transforms)\nval_dataset = LungDiseaseDataset(val_paths, val_labels, transform=val_test_transforms)\ntest_dataset = LungDiseaseDataset(test_paths, test_labels, transform=val_test_transforms)\n\n# Create data loaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Data loader batch outputs \nimages, labels = next(iter(train_loader))\nprint(f\"Batch shape: {images.shape}, Labels shape: {labels.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Class Imbalance Handling**","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport torch.nn as nn\nimport numpy as np\n\n# Compute class weights for dealing with class imbalance\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n# Loss function with class weights\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Optimization**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Optimizer and learning rate scheduler\noptimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-4)  # Weight decay for regularization\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\ncurrent_lr = scheduler.optimizer.param_groups[0]['lr']\nprint(f\"Current learning rate: {current_lr}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Training, Validation, and Early Stopping with TensorBoard Integration**","metadata":{}},{"cell_type":"code","source":"import time\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Function to train and validate the model\ndef train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=50, patience=12):\n    writer = SummaryWriter()  # TensorBoard for visualization\n    \n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    patience_counter = 0  # For early stopping\n\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch + 1}/{num_epochs}')\n        print('-' * 20)\n\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n\n        for inputs, labels in train_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n\n        train_losses.append(epoch_loss)\n        train_accs.append(epoch_acc.cpu())\n\n        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_corrects = 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                val_corrects += torch.sum(preds == labels.data)\n\n        val_loss = val_loss / len(val_loader.dataset)\n        val_acc = val_corrects.double() / len(val_loader.dataset)\n\n        val_losses.append(val_loss)\n        val_accs.append(val_acc.cpu())\n\n        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n        writer.add_scalars('Loss', {'train': epoch_loss, 'val': val_loss}, epoch)\n\n        # Early stopping\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_model_wts = model.state_dict()\n            patience_counter = 0  \n        else:\n            patience_counter += 1\n        \n        if patience_counter >= patience:\n            print(\"Early stopping triggered\")\n            break\n\n        # Step scheduler based on validation loss\n        scheduler.step(val_loss)\n\n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, train_losses, val_losses, train_accs, val_accs\n\n# Model training\nmodel, train_losses, val_losses, train_accs, val_accs = train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=50)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing Training and Validation Loss and Accuracy Curves**","metadata":{}},{"cell_type":"code","source":"def plot_loss_and_accuracy(train_losses, val_losses, train_accs, val_accs):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Plot loss\n    ax1.plot(train_losses, label='Train Loss')\n    ax1.plot(val_losses, label='Validation Loss')\n    ax1.set_title('Loss Curve')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n\n    # Plot accuracy\n    ax2.plot(train_accs, label='Train Accuracy')\n    ax2.plot(val_accs, label='Validation Accuracy')\n    ax2.set_title('Accuracy Curve')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n\n    plt.show()\n\n# Plot results after training\nplot_loss_and_accuracy(train_losses, val_losses, train_accs, val_accs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Evaluation**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Evaluate model on the test set and compute metrics\ndef test_model_with_metrics(model, test_loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n\n    # Collect predictions and ground truths\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    # Compute precision, recall, and F1-score\n    precision = precision_score(y_true, y_pred, average='weighted')  # Weighted to account for class imbalance\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n\n    print(f\"Precision (weighted): {precision:.4f}\")\n    print(f\"Recall (weighted): {recall:.4f}\")\n    print(f\"F1 Score (weighted): {f1:.4f}\")\n\n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=label_dict.keys()))\n\n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_dict.keys(), yticklabels=label_dict.keys())\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n# Evaluate model on the test set\ntest_model_with_metrics(model, test_loader)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained model\nmodel_save_path = 'resnet18_lung_disease_model.pth'\ntorch.save(model.state_dict(), model_save_path)\n\n# confirm the save\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}